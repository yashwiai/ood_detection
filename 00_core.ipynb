{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "> Documentation of the functions used in building the OOD Metric Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual overview of how the pipeline looks like. \n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A[Feature\\nEmbeddings] --> C{OOD Detection}\n",
    "  B[In Distribution\\nLabels] --> C\n",
    "  C --> F[Uncertainty Score]\n",
    "  F --> D[Out of Distribtuion]\n",
    "  F --> E[In Distribtuion]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_mean_and_covariance(\n",
    "    embdedding: np.ndarray, # (n_sample, n_dim) n_sample - sample size of training set, n_dim - dimension of the embedding\n",
    "    labels: np.ndarray, # (n_sample, ) n_sample - sample size of training set\n",
    ") -> Tuple[np.ndarray, np.ndarray]: # Mean of dimension (n_dim, ) and Covariance matrix of dimension(n_dim, n_dim)\n",
    "    \"\"\"Computes class-specific means and a shared covariance matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_dim = embdedding.shape[1]\n",
    "    class_ids = np.unique(labels)\n",
    "    \n",
    "    covariance = np.zeros((n_dim, n_dim)) \n",
    "    means = []\n",
    "\n",
    "    def f(covariance, class_id):\n",
    "        mask = np.expand_dims(labels == class_id, axis=-1) # to compute mean/variance use only those which belong to current class_id\n",
    "        data = embdedding * mask\n",
    "        mean = np.sum(data, axis=0) / np.sum(mask)\n",
    "        diff = (data - mean) * mask\n",
    "        covariance += np.matmul(diff.T, diff)\n",
    "        return covariance, mean\n",
    "\n",
    "    for class_id in class_ids:\n",
    "        covariance, mean = f(covariance, class_id)\n",
    "        means.append(mean)\n",
    "        \n",
    "    covariance = covariance / len(labels)\n",
    "    return np.stack(means), covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_mahalanobis_distance(\n",
    "    embdedding: np.ndarray, # Embdedding of dimension (n_sample, n_dim)\n",
    "    means: np.ndarray, # A matrix of size (num_classes, n_dim), where the ith row corresponds to the mean of the fitted Gaussian distribution for the i-th class.\n",
    "    covariance: np.ndarray # The shared covariance matrix of the size (n_dim, n_dim)\n",
    ") -> np.ndarray: # A matrix of size (n_sample, n_class) where the (i, j) element corresponds to the Mahalanobis distance between i-th sample to the j-th class Gaussian.\n",
    "    \"\"\"Computes Mahalanobis distance between the input and the fitted Guassians. The Mahalanobis distance (Mahalanobis, 1936) is defined as\n",
    "\n",
    "    $$distance(x, mu, sigma) = sqrt((x-\\mu)^T \\sigma^{-1} (x-\\mu))$$\n",
    "\n",
    "    where `x` is a vector, `mu` is the mean vector for a Gaussian, and `sigma` is\n",
    "    the covariance matrix. We compute the distance for all examples in `embdedding`,\n",
    "    and across all classes in `means`.\n",
    "\n",
    "    Note that this function technically computes the squared Mahalanobis distance\n",
    "    \"\"\"\n",
    "    \n",
    "    covariance_inv = np.linalg.pinv(covariance)\n",
    "    maha_distances = []\n",
    "\n",
    "    def maha_dist(x, mean):\n",
    "        # NOTE: This computes the squared Mahalanobis distance.\n",
    "        diff = x - mean\n",
    "        return np.einsum(\"i, ij, j->\", diff, covariance_inv, diff)\n",
    "\n",
    "    for x in embdedding:\n",
    "        arr = []\n",
    "        for mean in means:\n",
    "            arr.append(maha_dist(x, mean))\n",
    "        arr = np.stack(arr)\n",
    "        maha_distances.append(arr)\n",
    "\n",
    "    return np.stack(maha_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Metric Computation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OODMetric:\n",
    "    \"\"\"OOD Metric Class that calculates the OOD scores for a batch of input embeddings.\n",
    "    Initialises the class by fitting the class conditional gaussian using training data\n",
    "    and the class independent gaussian using training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_embdedding: np.ndarray, # An array of size (n_sample, n_dim) where n_sample is the sample size of training set, n_dim is the dimension of the embedding.\n",
    "                 train_labels: np.ndarray # An array of size (n_train_sample, )\n",
    "                ):\n",
    "        self.means, self.covariance = compute_mean_and_covariance(train_embdedding, train_labels)\n",
    "        self.means_bg, self.covariance_bg = compute_mean_and_covariance(train_embdedding, np.zeros_like(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def compute_rmd(\n",
    "    self:OODMetric,\n",
    "    embdedding: np.ndarray # An array of size (n_sample, n_dim), where n_sample is the sample size of the test set, and n_dim is the size of the embeddings.\n",
    ") -> np.ndarray:  # An array of size (n_sample, ) where the ith element corresponds to the ood score of the ith data point.\n",
    "    \"\"\"This function computes the OOD score using the mahalanobis distance\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = compute_mahalanobis_distance(embdedding, self.means, self.covariance)\n",
    "    distances_bg = compute_mahalanobis_distance(embdedding, self.means_bg, self.covariance_bg)\n",
    "\n",
    "    rmaha_distances = np.min(distances, axis=-1) - distances_bg[:, 0]\n",
    "    return rmaha_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding = np.random.standard_normal((32, 2048))\n",
    "train_labels = np.random.randint(low=0, high=5, size=(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(train_embedding), np.ndarray) # check that embeddings are a numpy array\n",
    "test_eq(type(train_labels), np.ndarray) # check that labels are numpy array\n",
    "test_eq(train_labels.dtype, int) # check that labels are integers only\n",
    "test_eq(train_labels.ndim, 1) # check that labels is one dimensional\n",
    "test_eq(train_embedding.shape[0], train_labels.shape[0]) # check n_samples are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood = OODMetric(train_embedding, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ood.means.shape[0], len(np.unique(train_labels))) # for each unique class, we should get one mean embedding\n",
    "test_eq(ood.means.shape[1], train_embedding.shape[1]) # size of mean vector should be the same of the size of embedding\n",
    "\n",
    "test_eq(ood.covariance.shape[0], train_embedding.shape[1]) # covariance matrix should be of size n_dim, n_dim\n",
    "test_eq(ood.covariance.shape[1], train_embedding.shape[1])\n",
    "\n",
    "test_eq(ood.means_bg.shape[0], 1)\n",
    "test_eq(ood.means_bg.shape[1], train_embedding.shape[1])\n",
    "\n",
    "test_eq(ood.covariance_bg.shape[0], train_embedding.shape[1])\n",
    "test_eq(ood.covariance_bg.shape[1], train_embedding.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.12871042e+11, -7.12871042e+11, -4.79065270e+11, -4.79065270e+11,\n",
       "       -3.67675472e+12, -3.67675472e+12, -3.67675472e+12, -3.67675472e+12,\n",
       "       -3.67675472e+12, -3.67675472e+12, -4.79065270e+11, -7.12871042e+11,\n",
       "       -3.67675472e+12, -7.12871042e+11, -7.12871042e+11, -3.67675472e+12,\n",
       "       -3.67675472e+12, -4.79065270e+11, -7.12871042e+11, -3.67675472e+12,\n",
       "       -4.79065270e+11, -7.12871042e+11, -3.67675472e+12, -3.67675472e+12,\n",
       "       -7.12871042e+11, -4.79065270e+11, -3.10000000e+01, -3.67675472e+12,\n",
       "       -7.12871042e+11, -3.67675472e+12, -3.67675472e+12, -7.12871042e+11])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood.compute_rmd(train_embedding) # testing on the train embedding itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.05771726e+13 -2.27167064e+12 -3.15972809e+13 -2.83203671e+13\n",
      " -8.02122950e+12  5.93743984e+12 -3.67828101e+11 -3.16653199e+13\n",
      " -1.36345972e+13  3.30371127e+12 -4.53200326e+12 -2.30555589e+13\n",
      " -7.09979234e+12 -7.09262508e+12 -3.14463722e+13 -1.02980640e+13\n",
      " -2.54963601e+13 -3.70189789e+13 -3.58977889e+13 -1.38412972e+12\n",
      " -2.00452276e+13  1.06730332e+13 -1.59431950e+13  1.93239518e+12\n",
      " -1.04444115e+11 -4.73320512e+13 -3.56125723e+13 -8.80546424e+12\n",
      " -2.96015093e+13  3.31277321e+11 -6.27571552e+12 -1.87232415e+13]\n"
     ]
    }
   ],
   "source": [
    "test_embedding = np.random.standard_normal((32, 2048)) # test embedding from same distribution\n",
    "scores = ood.compute_rmd(test_embedding)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(scores.shape[0], test_embedding.shape[0])\n",
    "test_eq(scores.ndim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.54006118e+12, -8.65916412e+12, -4.73688435e+12, -1.60718635e+12,\n",
       "       -1.72445557e+13, -3.36838664e+12, -6.60664867e+12, -9.13608411e+12,\n",
       "       -1.23727464e+13, -5.85111616e+12, -1.31499281e+12,  1.90948607e+12,\n",
       "       -3.02551028e+12, -1.46485214e+13, -1.22621818e+13, -2.71230431e+12,\n",
       "       -2.14953115e+12, -1.49131895e+13, -7.76931721e+12, -5.08826447e+12,\n",
       "       -9.76403621e+12, -1.05555622e+13, -1.05103160e+13, -6.48915780e+12,\n",
       "       -5.34682692e+12, -1.40335367e+13, -4.06784378e+12, -5.02260109e+12,\n",
       "       -6.08178220e+12, -5.52765883e+12,  1.59961404e+12, -1.56397685e+13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = np.random.uniform(size=(32, 2048)) # test embedding from different distribution\n",
    "ood.compute_rmd(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
